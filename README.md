# Adversarial Attack and Defense Techniques
This repository is purposed to study couples of adversarial attack and defense methods not only by reading the papers but by exploring their actual code implementations.

# Papers

- [x] Explaining and Harnessing Adversarial Examples (ICLR 2015)
- [x] Towards Evaluating the Robustness of Neural Networks (S&P 2017)
- [x] Towards Deep Learning Models Resistant to Adversarial Attacks (ICLR 2018)
- [x] Adversarial Examples Are Not Bugs, They Are Features (NIPS 2019)
- [x] Adversarial Reprogramming of Neural Networks
- [x] Hacking Neural Networks: A Short Introduction
- [x] Adversarial Examples in the Physical World
- [x] Fine-Pruning: Defending Against Backdooring Attacks on DNN
- [ ] Certified Robustness to Adversarial Examples with Differential Privacy (S&P 2019)
- [ ] Obfuscated Gradients Give a False Sense of Security (ICML 2018)
- [ ] Constructing Unrestricted Adversarial Examples with Generative Models (NIPS 2018)
- [ ] Deep Leackage from Gradients
- [ ] Adversarial Attacks: Attacks and Defenses for Deep Learning
- [ ] Adversarial Attacks and Defences Competition
- [ ] Audio Adversarial Examples: Targeted Attacks on Speech-to-Text


# Codes

**Attack**
- [x] Fast Gradient Sign Method
- [ ] One-step Target Class Method
- [ ] Basic Iterative Method
- [ ] Iterative Target Class Method (Iterative Least-Likely Class Method)
- [ ] Projected Gradient Descent
- [ ] CW Attack
- [ ] Shadow Attack
- [ ] BPDA
- [ ] Constructing Unrestricted Adversarial Examples with Generative Models
- [ ] Advesarial Reprogramming of Neural Networks

**Defence**
- [ ] PGD Adversarial Training
- [ ] Pixel Differential Privacy

**MORE**
* [cleverhans](https://github.com/cleverhans-lab/cleverhans)
* [advertorch](https://github.com/BorealisAI/advertorch)